devtools::load_all()
view(combined_posts)
test <- combined_posts
View(test)
getwd()
sys.which("R")
Sys.which("R")
######################### UPDATE DUCKDB DATABASE ############################
# Hourly cron job to update DuckDB and RDS storage
# Last updated: 2025-05-02 15:03:45
# Author: tomh1988-8
rm(list = ls())
gc() # Clean up memory before starting
########################## LIBRARIES ###########################################
library(tidyverse)
library(lubridate)
library(httr)
library(here)
library(furrr)
library(purrr)
library(duckdb)
library(ellmer) # For chat_ollama function
########################## DATABASE CONFIGURATION ############################
# Load database path from config file or use default
if (file.exists(here("data", "duckdb_config.rds"))) {
config <- readRDS(here("data", "duckdb_config.rds"))
db_path <- config$db_path
} else {
db_path <- here("data", "vibe_check.duckdb")
}
# Log start of process
cat("======= X/Twitter Vibe Check Update =======\n")
cat("Started at:", format(Sys.time(), "%Y-%m-%d %H:%M:%S"), "\n")
cat("Database path:", db_path, "\n\n")
########################## LOAD EXISTING DATA ################################
# Load the current RDS file - this is our main data source of record
cat("Loading existing data from RDS...\n")
combined_posts <- readRDS(file = here("data", "combined_posts.rds"))
old_unique_ids <- combined_posts$unique_id
cat("Loaded", nrow(combined_posts), "existing posts from RDS\n\n")
# Connect to DuckDB - keep connection open for entire process
# The conventional approach is to open once and close at the end
cat("Connecting to DuckDB database...\n")
con <- dbConnect(duckdb::duckdb(), dbdir = db_path)
# Verify database connection and get existing IDs
tryCatch(
{
# Get existing IDs from database for deduplication
existing_ids <- dbGetQuery(con, "SELECT unique_id FROM posts")$unique_id
cat("Successfully retrieved", length(existing_ids), "IDs from database\n")
# Combine IDs from both sources for complete deduplication
all_existing_ids <- unique(c(existing_ids, old_unique_ids))
},
error = function(e) {
# If database query fails, fall back to RDS data only
cat("Note: Could not retrieve IDs from database. Using only RDS data.\n")
cat("Error details:", conditionMessage(e), "\n")
all_existing_ids <- old_unique_ids
}
)
cat(
"Total unique post IDs for deduplication:",
length(all_existing_ids),
"\n\n"
)
########################## DATA ACQUISITION ##################################
cat("Reading new data from source files...\n")
# Define the folder path
folder_path <- "C:/Users/TomHun/OneDrive - City & Guilds/Documents/Code/R/vibe_check/backend_x_scraper/output"
# Read the data from CSV files
csv_files <- list.files(folder_path, pattern = "\\.csv$", full.names = TRUE)
csv_files <- csv_files[!grepl("urls", csv_files)]
# Read CSV files with consistent date handling
df_list <- map(csv_files, function(file) {
# Read file and handle dates consistently
df <- read_csv(file, show_col_types = FALSE)
# Handle date format inconsistencies if needed
if ("Created At" %in% colnames(df) && !inherits(df$`Created At`, "POSIXct")) {
df <- df %>%
mutate(`Created At` = parse_datetime(as.character(`Created At`)))
}
return(df)
})
names(df_list) <- basename(csv_files)
# Read URLs file separately
urls_file <- file.path(folder_path, "urls.csv")
urls <- read_csv(file = urls_file, show_col_types = FALSE)
# Combine all dataframes
updated_posts <- bind_rows(df_list)
cat("Read", length(csv_files), "files with", nrow(updated_posts), "posts\n\n")
########################## CLEAN AND FILTER ##################################
cat("Cleaning and filtering data...\n")
# Clean column names and data structure
updated_posts <- updated_posts |>
select(-c(`Tweet ID`, Likes, Retweets, Replies)) |>
select(
tag,
created_at = `Created At`,
Text,
post_url = `Tweet URL`,
everything()
) |>
rename_with(tolower) |>
mutate(unique_id = str_extract(post_url, "(?<=/status/)[0-9]+")) |>
filter(!is.null(text) & !is.na(text))
# Filter for only new posts (deduplication)
updated_posts <- updated_posts |>
filter(!unique_id %in% all_existing_ids)
# Check if we have new data
if (nrow(updated_posts) == 0) {
cat("No new posts found. Exiting.\n")
dbDisconnect(con, shutdown = TRUE)
quit(save = "no")
}
cat("Found", nrow(updated_posts), "new posts to process\n\n")
########################## FEATURE ENGINEERING ###############################
cat("Engineering features for new posts...\n")
# Calculate post length statistics
updated_posts <- updated_posts |>
mutate(
# Calculate post length with spaces (standard readable length)
post_length = nchar(text),
# Calculate post length without spaces (sometimes useful for analysis)
post_length_no_spaces = nchar(gsub("\\s+", "", text)),
# Calculate post length category for potential grouping
post_length_category = case_when(
post_length < 50 ~ "very short",
post_length < 100 ~ "short",
post_length < 200 ~ "medium",
post_length < 280 ~ "long",
TRUE ~ "very long"
)
)
########################## DATE INFO #######################################
cat("Processing date features...\n")
updated_posts <- updated_posts |>
mutate(
calendar_year = year(created_at),
financial_year = if_else(
month(created_at) >= 4,
year(created_at) + 1,
year(created_at)
),
calendar_quarter = quarter(created_at),
financial_quarter = (((month(created_at) - 4) %% 12) %/% 3) + 1,
month = month(created_at),
day_of_week = wday(created_at, label = TRUE, abbr = FALSE),
weekend_weekday = if_else(
wday(created_at) %in% c(1, 7),
"weekend",
"weekday"
),
hour_of_day = hour(created_at),
season = case_when(
month(created_at) %in% c(12, 1, 2) ~ "winter",
month(created_at) %in% c(3, 4, 5) ~ "spring",
month(created_at) %in% c(6, 7, 8) ~ "summer",
month(created_at) %in% c(9, 10, 11) ~ "autumn",
TRUE ~ NA_character_
),
# Calendar year-quarter variable
year_quarter = paste0(calendar_year, "-Q", calendar_quarter),
# For financial year-quarter, first compute the lower bound of the financial year:
financial_year_lower = if_else(
month(created_at) >= 4,
year(created_at),
year(created_at) - 1
),
# Create a financial year label like "2015-16":
financial_year_label = paste0(
financial_year_lower,
"-",
str_sub(as.character(financial_year_lower + 1), 3, 4)
),
# Combine with the financial quarter
financial_year_quarter = paste0(
financial_year_label,
"_Q",
financial_quarter
)
)
########################## POST URL EXTRACTIONS ###############################
cat("Extracting post metadata...\n")
updated_posts <- updated_posts |>
mutate(
# Extract the account name: characters after "https://x.com/" and before the next "/"
poster_account = str_extract(post_url, "(?<=https://x\\.com/)[^/]+"),
# Extract the tweet id: digits following "/status/"
unique_id = str_extract(post_url, "(?<=/status/)[0-9]+"),
# Create from_us: "y" if poster_account is cityandguilds, otherwise "n"
from_us = if_else(tolower(poster_account) == "cityandguilds", "y", "n")
)
########################## HASHTAG EXTRACTIONS ################################
cat("Processing hashtags...\n")
updated_posts <- updated_posts %>%
# Overwrite hashtags with a list of individual, trimmed, and lowercased hashtag strings
mutate(
hashtags = map(
hashtags,
~ if (is.na(.x)) {
character(0)
} else {
map_chr(str_split(.x, pattern = ",")[[1]], ~ str_to_lower(str_trim(.x)))
}
)
) %>%
# Compute counts and categorical variables for hashtags
mutate(
num_hashtags = map_int(hashtags, length),
num_hashtags_excl = map_int(hashtags, ~ sum(.x != "#cityandguilds")),
hashtag_category = case_when(
num_hashtags_excl == 0 ~ "none",
num_hashtags_excl == 1 ~ "one",
num_hashtags_excl > 1 ~ "more than one"
),
extended_hashtag_category = case_when(
num_hashtags_excl == 0 ~ "none",
num_hashtags_excl == 1 ~ "one",
num_hashtags_excl > 10 ~ "more than ten",
num_hashtags_excl > 5 ~ "more than five",
num_hashtags_excl > 1 ~ "more than one"
)
)
########################## MENTIONS EXTRACTIONS ###############################
cat("Processing mentions...\n")
updated_posts <- updated_posts |>
# Overwrite mentions with a list of individual, trimmed, and lowercased mention strings
mutate(
mentions = map(
mentions,
~ if (is.na(.x)) {
character(0)
} else {
map_chr(str_split(.x, pattern = ",")[[1]], ~ str_to_lower(str_trim(.x)))
}
)
) |>
# Compute counts and categorical variables for mentions
mutate(
num_mentions = map_int(mentions, length),
num_mentions_excl = map_int(mentions, ~ sum(.x != "@cityandguilds")),
mention_category = case_when(
num_mentions_excl == 0 ~ "none",
num_mentions_excl == 1 ~ "one",
num_mentions_excl > 1 ~ "more than one"
),
extended_mention_category = case_when(
num_mentions_excl == 0 ~ "none",
num_mentions_excl == 1 ~ "one",
num_mentions_excl > 10 ~ "more than ten",
num_mentions_excl > 5 ~ "more than five",
num_mentions_excl > 1 ~ "more than one"
)
)
########################## URL EXTRACTIONS ###################################
cat("Processing URLs...\n")
updated_posts <- updated_posts |>
# Process URLs: split by comma, trim whitespace
mutate(
urls = map(
urls,
~ if (is.na(.x)) {
character(0)
} else {
map_chr(str_split(.x, pattern = ",")[[1]], ~ str_trim(.x))
}
)
) |>
# Compute counts and categorical variables for URLs
mutate(
num_urls = map_int(urls, length),
url_category = case_when(
num_urls == 0 ~ "none",
num_urls == 1 ~ "one",
num_urls > 1 ~ "more than one"
),
extended_url_category = case_when(
num_urls == 0 ~ "none",
num_urls == 1 ~ "one",
num_urls > 10 ~ "more than ten",
num_urls > 5 ~ "more than five",
num_urls > 1 ~ "more than one"
)
)
########################## EXTRA POST CONTENT ################################
cat("Creating add-on metrics...\n")
updated_posts <- updated_posts |>
mutate(
# Total add-ons: sum of hashtags, mentions, and urls counts
total_add_ons = num_hashtags + num_mentions + num_urls,
# Basic category: "none", "one", or "more than one"
add_on_category = case_when(
total_add_ons == 0 ~ "none",
total_add_ons == 1 ~ "one",
total_add_ons > 1 ~ "more than one"
),
# Extended category with finer thresholds
extended_add_on_category = case_when(
total_add_ons == 0 ~ "none",
total_add_ons == 1 ~ "one",
total_add_ons > 10 ~ "more than ten",
total_add_ons > 5 ~ "more than five",
total_add_ons > 1 ~ "more than one"
)
)
########################## EXTENDED SENTIMENT #################################
cat("Performing sentiment analysis with LLM...\n")
# Start a chat session with the LLM
system_prompt <- "You are an expert tweet-type annotator for organizational tweets and mentions.
Classify each prompt into exactly one of the following categories.
Your answer must be strictly one word and must be exactly one of the options listed below (no other words allowed).
Definitions:
Announcement: Official news, updates, launches, or changes.
Advertisement: Promotional content, product or service pitches, sales messages.
Opinion: An expression of a personal or organizational view, stance, or perspective.
Question: A request for information, input, or clarification.
Response: A reply to another tweet or message, addressing a previous statement or inquiry.
Engagement: Conversation starters, polls, or other efforts to interact with the community.
Event: Information about an upcoming or past event, webinar, or conference.
Support: Offering help, troubleshooting, or customer service.
Recruitment: Job postings, hiring messages, or internship opportunities.
Recognition: Acknowledgment or highlighting of achievements, milestones, or individuals.
Information: Sharing facts, tips, statistics, or educational content.
Alert: Warnings, urgent messages, or time-sensitive updates.
Complaint: Expression of dissatisfaction or reporting a problem.
Praise: Expressions of positive feedback, commendations, or endorsements.
Request: Asking for a specific action, resource, or outcome.
Thanks: Explicit expressions of gratitude.
For each prompt, reply with only one of these options and nothing else."
chat <- chat_ollama(
model = "gemma3:4b-it-qat",
system_prompt = system_prompt
)
# Loop through posts for sentiment analysis
updated_posts <- updated_posts |>
mutate(
sentiment_extended = map_chr(text, function(post_text) {
chat$set_turns(list(Turn("system", system_prompt)))
chat$chat(post_text)
})
)
# Combine sentiment values
updated_posts <- updated_posts |>
mutate(sentiment_combined = paste(sentiment, sentiment_extended, sep = "-"))
########################## URL EXPANSION #####################################
cat("Expanding shortened URLs...\n")
# Enhanced URL expansion function with retries
expand_url <- function(url, max_attempts = 3) {
for (i in 1:max_attempts) {
res <- try(HEAD(url, timeout(5)), silent = TRUE)
if (!inherits(res, "try-error")) return(res$url)
Sys.sleep(1) # Wait before retry
}
return(url) # Return original if all attempts fail
}
# Set up parallel processing with appropriate number of workers
available_cores <- parallel::detectCores() - 1
available_cores <- max(1, available_cores) # Ensure at least 1 core
plan(multisession, workers = min(available_cores, 3))
# Apply the expansion function to each element of the urls list-column in parallel
updated_posts <- updated_posts |>
mutate(
expanded_urls = future_map(
urls,
~ if (length(.x) == 0) {
character(0)
} else {
future_map_chr(.x, expand_url)
},
.options = furrr_options(seed = TRUE)
)
)
# Clean up parallel workers
plan(sequential)
########################## DATABASE UPDATE PREPARATION #######################
cat("\nPreparing data for database update...\n")
# Get the current maximum post_id
max_post_id <- tryCatch(
{
dbGetQuery(
con,
"SELECT COALESCE(MAX(post_id), 0) AS max_id FROM posts"
)$max_id
},
error = function(e) {
# If query fails, use RDS data as fallback
cat("Error getting max post_id:", conditionMessage(e), "\n")
cat("Using max ID from RDS data as fallback\n")
max_id <- if (!is.null(combined_posts$post_id))
max(combined_posts$post_id) else 0
return(max_id)
}
)
cat("Current maximum post_id:", max_post_id, "\n")
# Add post_id as sequential integers starting after the current maximum
updated_posts <- updated_posts %>%
mutate(post_id = row_number() + max_post_id) %>%
select(post_id, everything()) # Move post_id to first column for clarity
########################## DATA UPDATES #####################################
# RDS UPDATE
cat("Updating RDS backup file...\n")
combined_posts_updated <- bind_rows(combined_posts, updated_posts)
saveRDS(combined_posts_updated, file = here("data", "combined_posts.rds"))
cat("RDS backup completed successfully\n")
# DATABASE UPDATE
cat("Preparing data for database insertion...\n")
# Convert list columns to JSON strings for storage in DuckDB
updated_posts_for_db <- updated_posts %>%
mutate(
hashtags = map_chr(hashtags, ~ jsonlite::toJSON(.x)),
mentions = map_chr(mentions, ~ jsonlite::toJSON(.x)),
urls = map_chr(urls, ~ jsonlite::toJSON(.x)),
expanded_urls = map_chr(expanded_urls, ~ jsonlite::toJSON(.x))
)
# Insert new data into DuckDB
cat("Inserting", nrow(updated_posts_for_db), "new posts into database...\n")
tryCatch(
{
dbAppendTable(con, "posts", updated_posts_for_db)
db_update_success <- TRUE
cat("Database update completed successfully\n")
},
error = function(e) {
# Handle database insertion error
db_update_success <- FALSE
cat("Database insertion error:", conditionMessage(e), "\n")
cat("Data is still preserved in RDS file\n")
}
)
# Close the database connection
cat("Closing database connection...\n")
dbDisconnect(con, shutdown = TRUE)
########################## COMPLETION LOGGING ################################
# Log update information
log_message <- sprintf(
"[%s] Added %d new posts to %s.\n",
format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
nrow(updated_posts),
if (db_update_success) "both DuckDB and RDS" else
"RDS only (DB update failed)"
)
# Write to log file
write(log_message, file = here("data", "update_log.txt"), append = TRUE)
# Final output
cat("\n======= UPDATE SUMMARY =======\n")
cat(log_message)
cat(
"Update process completed at",
format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
"\n"
)
cat("==============================\n")
devtools::load_all()
test <_ combined_posts
test <- combined_posts
########################## ALL CLEAR! ##########################################
rm(list = ls())
########################## LIBRARIES ###########################################
library(tidyverse)
library(lubridate)
library(httr)
library(here)
# Add furrr for parallel processing
library(furrr)
library(purrr)
library(ellmer)
folder_path <- "C:/Users/TomHun/OneDrive - City & Guilds/Documents/Code/R/vibe_check/backend_x_scraper/output"
csv_files <- list.files(folder_path, pattern = "\\.csv$", full.names = TRUE)
# Isolate the csv files
csv_files <- csv_files[!grepl("urls", csv_files)]
# Read the chosen csvs into a list based on their file names with parsing fix
df_list <- map(
csv_files,
~ {
df <- read_csv(.x, show_col_types = FALSE)
# Ensure Created At is always a datetime
if (
"Created At" %in% colnames(df) && !inherits(df$`Created At`, "POSIXct")
) {
df$`Created At` <- parse_datetime(df$`Created At`)
}
return(df)
}
)
View(df_list)
test <- combined_posts
View(test)
